---
title: "Cuaderno con Conceptos de IA"
author: "Diego Iturrieta"
date: "2025-04-15"
output:
  html_document: 
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ğŸ“˜ Glosario PrÃ¡ctico y Conceptual sobre LLMs para Investigadores Sociales

Este glosario tiene como objetivo entregar definiciones claras, ejemplos aplicados al anÃ¡lisis discursivo, y fundamentos prÃ¡cticos para comprender el uso de modelos de lenguaje de gran tamaÃ±o (LLMs) en investigaciÃ³n polÃ­tica y social.

---

## ğŸ”‘ Conceptos Fundamentales

### ğŸ§  LLM (Large Language Model)
**DefiniciÃ³n:**  
Modelo de lenguaje con miles de millones de parÃ¡metros, entrenado sobre grandes corpus de texto para entender y generar lenguaje natural.

**Ejemplo en polÃ­tica:**  
Puedes usar un LLM como LLaMA 3.1 para clasificar discursos de alcaldes segÃºn si promueven ideas de "mano dura" o no.

---

### ğŸ§® ParÃ¡metros
**DefiniciÃ³n:**  
Son los â€œpesosâ€ que el modelo ajusta durante su entrenamiento. Determinan cÃ³mo el modelo interpreta el texto.

**Ejemplo:**  
LLaMA 3.1 de 4B tiene 4.000 millones de parÃ¡metros. Modelos mÃ¡s grandes como GPT-3 pueden tener 175B.

---

### ğŸ—£ï¸ Inferencia
**DefiniciÃ³n:**  
Fase en la que usas el modelo ya entrenado para predecir, clasificar o generar texto.

**Ejemplo:**  
Cuando pasas un discurso al modelo y te responde: "SÃ­, este discurso promueve polÃ­ticas punitivas", estÃ¡s haciendo inferencia.

---

### ğŸ”„ Fine-tuning
**DefiniciÃ³n:**  
Reentrenar un modelo preexistente con tus propios datos para que se adapte a un dominio o tarea especÃ­fica.

**Ejemplo:**  
Usar 300 discursos etiquetados por ti para ajustar LLaMA y que aprenda a reconocer con mÃ¡s precisiÃ³n ideas de "militarizaciÃ³n".

---

## ğŸ“¦ Componentes del Proceso

### ğŸ“„ Token
**DefiniciÃ³n:**  
Unidad mÃ­nima de texto que el modelo entiende. Puede ser una palabra, parte de una palabra o incluso signos de puntuaciÃ³n.

**Ejemplo:**  
"Seguridad pÃºblica" puede dividirse en tokens como ["Seg", "uridad", " pÃºblica"] dependiendo del modelo.

---

### ğŸ“ Embedding
**DefiniciÃ³n:**  
RepresentaciÃ³n numÃ©rica de un texto (token, frase, oraciÃ³n) en un espacio vectorial. Permite comparar semÃ¡nticamente distintos textos.

**Ejemplo:**  
Dos discursos con ideas similares tendrÃ¡n embeddings cercanos entre sÃ­, lo que permite hacer clustering.

---

### ğŸ§  Attention
**DefiniciÃ³n:**  
Mecanismo que permite al modelo "poner atenciÃ³n" en partes relevantes del texto cuando procesa una palabra.

**Ejemplo:**  
En la frase "La alcaldesa propuso aumentar la dotaciÃ³n policial", el modelo presta atenciÃ³n a "aumentar" y "policial" para entender el foco del discurso.

---

### ğŸ§© Transformer
**DefiniciÃ³n:**  
Arquitectura base de casi todos los LLMs actuales. Introducida en el paper "Attention is All You Need".

**Ejemplo:**  
GPT, BERT y LLaMA son todos modelos basados en transformers.

---

### ğŸ—‚ï¸ Prompt
**DefiniciÃ³n:**  
InstrucciÃ³n o entrada que das al modelo para obtener una respuesta.

**Ejemplo:**  
Prompt simple:  
> "Clasifica el siguiente discurso segÃºn si usa una narrativa de mano dura: {discurso}"

---

### ğŸ” Prompt Engineering
**DefiniciÃ³n:**  
TÃ©cnica para diseÃ±ar prompts mÃ¡s efectivos, claros y orientados a la tarea deseada.

**Ejemplo:**  
Prompt mejorado:
> "A partir del siguiente texto, indica si el discurso refleja una polÃ­tica de mano dura. Justifica tu respuesta. Texto: {discurso}"

---

## âš™ï¸ En entorno de desarrollo

### ğŸ’» Ollama
**DefiniciÃ³n:**  
Herramienta que permite correr modelos LLM localmente en tu computador (como LLaMA) de forma optimizada.

**Ejemplo:**  
EstÃ¡s usando `ollama run llama3.1` para hacer inferencia directamente desde CMD.

---

### ğŸ Python
**DefiniciÃ³n:**  
Lenguaje de programaciÃ³n con el que puedes integrar modelos, procesar datos y automatizar tu anÃ¡lisis temÃ¡tico.

**Ejemplo:**  
Leer discursos desde Excel, limpiarlos con `pandas`, y enviarlos al modelo con una peticiÃ³n vÃ­a `subprocess` u `Ollama API`.

---

### ğŸ“Š Pandas
**DefiniciÃ³n:**  
LibrerÃ­a de Python para anÃ¡lisis y manipulaciÃ³n de datos estructurados (tablas, Excel, CSV).

**Ejemplo:**  
Cargar tu base de discursos en Excel y recorrer cada discurso con un loop para hacer inferencia.

---

### ğŸ“š Transformers (librerÃ­a)
**DefiniciÃ³n:**  
LibrerÃ­a de `Hugging Face` que permite trabajar con miles de modelos preentrenados. Puedes hacer clasificaciÃ³n, embeddings, QA, etc.

**Ejemplo:**  
Una vez que lleves el modelo a Python, podÃ©s usar `transformers.pipeline("text-classification")` para hacer clasificaciones automÃ¡ticas.

---

## ğŸ§ª Otros tÃ©rminos relevantes

### âš™ï¸ Quantization
**DefiniciÃ³n:**  
TÃ©cnica para reducir el tamaÃ±o de un modelo bajando la precisiÃ³n de los parÃ¡metros (por ejemplo, de 16 bits a 4 bits).

**Ejemplo:**  
Tu modelo LLaMA 4B probablemente estÃ© cuantizado a 4 bits, lo que permite correrlo en GPU con 8 GB VRAM.

---

### ğŸ§  LoRA (Low-Rank Adaptation)
**DefiniciÃ³n:**  
TÃ©cnica ligera para hacer fine-tuning sin modificar todos los parÃ¡metros del modelo.

**Ejemplo:**  
PodrÃ­as ajustar tu modelo a discursos chilenos sin necesitar grandes recursos computacionales.

---

### ğŸ“Œ Zero-shot / Few-shot
**DefiniciÃ³n:**  
- Zero-shot: el modelo responde sin ver ejemplos.
- Few-shot: el modelo recibe 1â€“5 ejemplos en el prompt para "aprender" el patrÃ³n.

**Ejemplo:**  
Zero-shot: "Â¿Este discurso es punitivista?"  
Few-shot: AÃ±ades 2 ejemplos antes del discurso objetivo.

---

### ğŸ“‚ Dataset
**DefiniciÃ³n:**  
ColecciÃ³n de datos estructurados que se usa para entrenar o validar modelos.

**Ejemplo:**  
Tu Excel de discursos es un dataset que podÃ©s usar para clasificaciÃ³n, fine-tuning o embeddings.

---

### ğŸ§  Overfitting
**DefiniciÃ³n:**  
Cuando el modelo se adapta demasiado a los datos de entrenamiento y pierde capacidad de generalizar.

**Ejemplo:**  
Si haces fine-tuning con muy pocos discursos, el modelo podrÃ­a funcionar solo con esos y fallar con nuevos.

---

### ğŸ”„ Pipeline
**DefiniciÃ³n:**  
Secuencia de pasos automatizados para procesar datos, pasarlos por un modelo, y guardar los resultados.

**Ejemplo:**  
Leer discurso âœ enviar a LLaMA âœ clasificar âœ guardar en Excel âœ repetir.

---

Â¿Quieres que te prepare ahora la segunda parte: el inicio del cuaderno con estructura Markdown + ejemplo de flujo para anÃ¡lisis temÃ¡tico con LLaMA y pandas?
